---
title: Optimisations for Computer Architecutes - Compilers - Computer Science
type: default
layout: page
child: Computer Science
fold: Compilers
---

The rapid evolution of computer architectures has also led ot an insatiable
demand for new compiler technology. Almost all high-performance systems take
advantage of the same two basic techniques: _parallelism_ and _memory
hierarchies_. Parallelism can be found at several levels: at the _instruction
level_, where multiple operations are executed simultaneously and at the
_processor level_, where different threads of the same application are run on
different processors. Memory hierarchies are a response to the basic limitation
that we can build very fast storage or very large storage, but not storage that
is both fast and large.

## Parallelism

All modern microprocesors exploit instruction-level parallelism. However, this
parallelism can be hidden from the programmer. Programs are written as if all
instructions were executed in sequence; the hardware dynamically checks for
dependencies in the sequential instruction stream and issues them in parallel
when possible. In some cases, the machines includes a hardware scheduler that
can change the instruction ordering to increase the parallelism in the program.
Whether the hardware reorders the instructions or not, compilers can rearrange
the instructions to make instruction-level parallelism more effective.

Instruction-level parallelism can also appear explicitly in the instruction set.
VLIW (Very Long Instruction Word) machines have instructions that can issue
multiple operations in parallel. The Intel IA64 is a well-known example of such
an architecture. All high-performance, general-purpiose microprocessors also
include instructions that can operate on a vector of data at the same time.
Compiler techniques have been developed to generate code automatically for such
machines from sequential programs.

Multiprocessors have also become prevalent; even personal computers often have
multiprocessors. Programmers can write multithreaded code for multiprocessors,
or parallel code can be automatically generated by a compiler from conventional
sequential programs. Such a compiler hides from the programmers the details of
finding parallelism in a program, distributing the computation across the
machine, and minimising synchronization and communication among the processors.
Many scientific-computing and engineering applications are computation-intensive
and can benefit greatly from parallel processing. Parallelization techniques
have been deployed to translate automatically sequential programs scientific
programs into multiprocessor code.

## Memory Hierarchies

A memory hierarchy consists of several levels of storage with different speeds
and sizes, with the level closest to the processor being the fastest but
smallest. The average memory-access time of a program is reduced if most of its
accesses are satisfied by the faster levels of the hierarchy. Both parallelism
and the existence of a memory hierarchy improve the potential performance of a
machine, but they must be harnessed effectively by the compiler to deliver real
performance on an application.

Memory hierarchies are found in all machines. A processor usually has a small
number of registers consisting of hundreds of bytes, several levels of caches
containing kilobytes to megabytes, physical memory containing megabytes to
gigabytes, and finally secondary storage that contains gigabytes and beyond.
Correspondingly, the speed of access between adjacent levels of the hierarchy
can differ by two or three orders of magnitude. The performance of a system is
often limited not by the speed of the processor but by the performance of the
memory susbsystem. While compilers traditionally focus on optimising the
processor execution, more emphasis is now placed on making the processor
execution, more emphasis is now placed on making the memory hierarchy more
effective.

Using registers effectively is probably the single most important problem in
optimising a program. Unlike registers that have to be managed explicitly in
software, caches and physical memories are hidden from the instruction set and
are managed by hardware. It has been found that cache-management policies
implemented by hardware are not effective in some cases, especially in
scientific code that has large data structures (arrays, typically). It is
possible to improve the effectiveness of the memory hierarchy by changing the
laoyut of the data, or changing the order of instructions accessing the data. We
can also change the layout of code to improve the effectiveness of instruction
caches.
